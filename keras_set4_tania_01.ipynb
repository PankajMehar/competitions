{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedShuffleSplit, ShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.regularizers import WeightRegularizer, l1, l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU, ELU, LeakyReLU\n",
    "from keras.callbacks import EarlyStopping, Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from libscores import pac_metric\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WatchlistCallback(Callback):\n",
    "    def __init__(self, watchlist, eval_metric, epoch_no=10):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.epoch_no = epoch_no\n",
    "        self.X, self.y = watchlist\n",
    "        self.eval_metric = eval_metric\n",
    "        self.scores = []\n",
    "        self.epochs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.epoch_no == 0:\n",
    "            y_pred = self.model.predict(self.X, verbose=0)\n",
    "            score = self.eval_metric(self.y, y_pred)\n",
    "            self.scores.append(score)\n",
    "            self.epochs.append(epoch)\n",
    "\n",
    "            print \"score: %0.6f (epoch no %d)\" % (score, epoch)  \n",
    "\n",
    "class StatusCallback(Callback):    \n",
    "    def __init__(self, epoch_total):\n",
    "        super(Callback, self).__init__()\n",
    "        self.progress = tqdm(total=epoch_total)\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.progress.update(1)\n",
    "\n",
    "    def __enter__(self): \n",
    "        return self\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self.progress.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_info(file_name):\n",
    "    result = []\n",
    "    \n",
    "    for line in file(file_name):\n",
    "        key, value = line.strip().split('=')\n",
    "        key = key.strip()\n",
    "        value = value.strip().strip(\"'\")\n",
    "        if value.isdigit():\n",
    "            value = int(value)\n",
    "        result.append((key, value))\n",
    "    \n",
    "    return dict(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feat_num': 47236,\n",
       " 'feat_type': 'Numerical',\n",
       " 'has_categorical': 0,\n",
       " 'has_missing': 0,\n",
       " 'is_sparse': 1,\n",
       " 'label_num': 95,\n",
       " 'metric': 'pac_metric',\n",
       " 'name': 'tania',\n",
       " 'target_num': 95,\n",
       " 'target_type': 'Binary',\n",
       " 'task': 'multilabel.classification',\n",
       " 'test_num': 44635,\n",
       " 'time_budget': 1200,\n",
       " 'train_num': 157599,\n",
       " 'usage': 'AutoML challenge 2014',\n",
       " 'valid_num': 22514}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "public = read_info('data/set4_tania/tania_public.info')\n",
    "public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dim = public['feat_num']\n",
    "\n",
    "def read_sparse_features(fine_name):\n",
    "    result = list()\n",
    "    for line in file(fine_name):\n",
    "        row = []\n",
    "        for el in line.strip().split(' '): \n",
    "            pos, value = el.split(':')\n",
    "            pos = int(pos) - 1\n",
    "            row.append((pos, float(value)))\n",
    "        result.append(row)\n",
    "\n",
    "    rnum = len(result)\n",
    "    X = scipy.sparse.dok_matrix((rnum, dim), dtype=np.float)\n",
    "    for idx, row in enumerate(result):\n",
    "        for pos, val in row:\n",
    "            X[idx, pos] = val\n",
    "\n",
    "    return scipy.sparse.csr_matrix(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_multilabels(file_name):\n",
    "    result = list()\n",
    "    for line in file(file_name):\n",
    "        row = [int(f) for f in line.strip().split(' ')]\n",
    "        result.append(row)\n",
    "    return scipy.sparse.csr_matrix(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y, X_valid, X_test = pickle.load(open('data/set4_tania/data.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "X = read_sparse_features('data/set4_tania/tania_train.data')\n",
    "y = read_multilabels('data/set4_tania/tania_train.solution')\n",
    "\n",
    "X_valid = read_sparse_features('data/set4_tania/tania_valid.data')\n",
    "X_test  = read_sparse_features('data/set4_tania/tania_test.data')\n",
    "\n",
    "print X.shape, y.shape, np.unique(y).shape\n",
    "print X_valid.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "pickle.dump([X, y, X_valid, X_test], open('data/set4_tania/data.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rows, cols = X.nonzero()\n",
    "per_col_count = np.bincount(cols)\n",
    "\n",
    "X = X[:, per_col_count > 5]\n",
    "X_valid = X_valid[:, per_col_count > 5]\n",
    "X_test = X_test[:, per_col_count > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=500, random_state=123)\n",
    "X = svd.fit_transform(X)\n",
    "X_valid = svd.transform(X_valid)\n",
    "X_test = svd.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler(with_mean=True, copy=False)\n",
    "X = scaler.fit_transform(X)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "EPOCHS = 1500\n",
    "n, y_dim = y.shape\n",
    "\n",
    "cv = ShuffleSplit(n, n_iter=1, train_size=train_size, random_state=1)\n",
    "train, test = next(cv.__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_svd = X[train]\n",
    "X_test_svd = X[test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(input_dim=X_train_svd.shape[1], output_dim=300, init='uniform')) \n",
    "model.add(Activation('tanh')) \n",
    "model.add(Dropout(0.2)) \n",
    "model.add(Dense(input_dim=300, output_dim=150, init='uniform')) \n",
    "model.add(Activation('sigmoid')) \n",
    "model.add(Dropout(0.1)) \n",
    "model.add(Dense(output_dim=y.shape[1], init='uniform')) \n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adagrad')\n",
    "\n",
    "eval_metric = partial(pac_metric, task='multilabel.classification')\n",
    "# score = pac_metric(y[test].toarray(), y_pred, task='multilabel.classification')\n",
    "\n",
    "watchlist = WatchlistCallback(watchlist=(X_test_svd, y[test].toarray()), \n",
    "                              eval_metric=eval_metric, epoch_no=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: -0.458257 (epoch no 0)\n",
      "score: -0.045074 (epoch no 1)\n",
      "score: 0.218930 (epoch no 2)\n",
      "score: 0.371278 (epoch no 3)\n",
      "score: 0.454003 (epoch no 4)\n",
      "score: 0.457272 (epoch no 5)\n",
      "score: 0.449189 (epoch no 6)\n",
      "score: 0.448582 (epoch no 7)\n",
      "score: 0.451857 (epoch no 8)\n",
      "score: 0.457130 (epoch no 9)\n",
      "score: 0.462126 (epoch no 10)\n",
      "score: 0.468638 (epoch no 11)\n",
      "score: 0.476160 (epoch no 12)\n",
      "score: 0.482081 (epoch no 13)\n",
      "score: 0.487642 (epoch no 14)\n",
      "score: 0.494570 (epoch no 15)\n",
      "score: 0.500458 (epoch no 16)\n",
      "score: 0.506436 (epoch no 17)\n",
      "score: 0.511700 (epoch no 18)\n",
      "score: 0.516824 (epoch no 19)\n",
      "score: 0.522586 (epoch no 20)\n",
      "score: 0.527302 (epoch no 21)\n",
      "score: 0.532023 (epoch no 22)\n",
      "score: 0.536366 (epoch no 23)\n",
      "score: 0.540775 (epoch no 24)\n",
      "score: 0.544793 (epoch no 25)\n",
      "score: 0.547970 (epoch no 26)\n",
      "score: 0.552781 (epoch no 27)\n",
      "score: 0.555928 (epoch no 28)\n",
      "score: 0.559756 (epoch no 29)\n",
      "score: 0.563463 (epoch no 30)\n",
      "score: 0.566471 (epoch no 31)\n",
      "score: 0.569765 (epoch no 32)\n",
      "score: 0.572619 (epoch no 33)\n",
      "score: 0.575505 (epoch no 34)\n",
      "score: 0.578291 (epoch no 35)\n",
      "score: 0.580814 (epoch no 36)\n",
      "score: 0.583892 (epoch no 37)\n",
      "score: 0.586147 (epoch no 38)\n",
      "score: 0.588985 (epoch no 39)\n",
      "score: 0.591495 (epoch no 40)\n",
      "score: 0.593735 (epoch no 41)\n",
      "score: 0.596246 (epoch no 42)\n",
      "score: 0.598446 (epoch no 43)\n",
      "score: 0.600318 (epoch no 44)\n",
      "score: 0.602237 (epoch no 45)\n",
      "score: 0.604143 (epoch no 46)\n",
      "score: 0.606258 (epoch no 47)\n",
      "score: 0.608072 (epoch no 48)\n",
      "score: 0.610448 (epoch no 49)\n",
      "score: 0.612078 (epoch no 50)\n",
      "score: 0.613880 (epoch no 51)\n",
      "score: 0.615412 (epoch no 52)\n",
      "score: 0.617612 (epoch no 53)\n",
      "score: 0.618799 (epoch no 54)\n",
      "score: 0.620204 (epoch no 55)\n",
      "score: 0.621922 (epoch no 56)\n",
      "score: 0.623485 (epoch no 57)\n",
      "score: 0.624966 (epoch no 58)\n",
      "score: 0.626084 (epoch no 59)\n",
      "score: 0.627700 (epoch no 60)\n",
      "score: 0.629267 (epoch no 61)\n",
      "score: 0.630733 (epoch no 62)\n",
      "score: 0.631807 (epoch no 63)\n",
      "score: 0.633176 (epoch no 64)\n",
      "score: 0.634602 (epoch no 65)\n",
      "score: 0.636221 (epoch no 66)\n",
      "score: 0.636811 (epoch no 67)\n",
      "score: 0.637818 (epoch no 68)\n",
      "score: 0.639115 (epoch no 69)\n",
      "score: 0.640321 (epoch no 70)\n",
      "score: 0.641259 (epoch no 71)\n",
      "score: 0.642356 (epoch no 72)\n",
      "score: 0.643287 (epoch no 73)\n",
      "score: 0.644747 (epoch no 74)\n",
      "score: 0.645801 (epoch no 75)\n",
      "score: 0.646899 (epoch no 76)\n",
      "score: 0.647690 (epoch no 77)\n",
      "score: 0.648781 (epoch no 78)\n",
      "score: 0.649765 (epoch no 79)\n",
      "score: 0.650673 (epoch no 80)\n",
      "score: 0.651414 (epoch no 81)\n",
      "score: 0.652431 (epoch no 82)\n",
      "score: 0.652941 (epoch no 83)\n",
      "score: 0.654350 (epoch no 84)\n",
      "score: 0.654874 (epoch no 85)\n",
      "score: 0.656135 (epoch no 86)\n",
      "score: 0.656979 (epoch no 87)\n",
      "score: 0.657629 (epoch no 88)\n",
      "score: 0.658452 (epoch no 89)\n",
      "score: 0.659436 (epoch no 90)\n",
      "score: 0.660006 (epoch no 91)\n",
      "score: 0.660807 (epoch no 92)\n",
      "score: 0.661501 (epoch no 93)\n",
      "score: 0.662074 (epoch no 94)\n",
      "score: 0.662965 (epoch no 95)\n",
      "score: 0.663491 (epoch no 96)\n",
      "score: 0.664215 (epoch no 97)\n",
      "score: 0.665053 (epoch no 98)\n",
      "score: 0.665614 (epoch no 99)\n",
      "score: 0.666115 (epoch no 100)\n",
      "score: 0.666735 (epoch no 101)\n",
      "score: 0.667541 (epoch no 102)\n",
      "score: 0.667972 (epoch no 103)\n",
      "score: 0.668758 (epoch no 104)\n",
      "score: 0.669413 (epoch no 105)\n",
      "score: 0.670055 (epoch no 106)\n",
      "score: 0.670818 (epoch no 107)\n",
      "score: 0.671231 (epoch no 108)\n",
      "score: 0.671930 (epoch no 109)\n",
      "score: 0.672319 (epoch no 110)\n",
      "score: 0.672965 (epoch no 111)\n",
      "score: 0.673462 (epoch no 112)\n",
      "score: 0.673794 (epoch no 113)\n",
      "score: 0.674320 (epoch no 114)\n",
      "score: 0.674918 (epoch no 115)\n",
      "score: 0.675432 (epoch no 116)\n",
      "score: 0.675895 (epoch no 117)\n",
      "score: 0.676331 (epoch no 118)\n",
      "score: 0.677094 (epoch no 119)\n",
      "score: 0.677742 (epoch no 120)\n",
      "score: 0.678165 (epoch no 121)\n",
      "score: 0.678481 (epoch no 122)\n",
      "score: 0.678912 (epoch no 123)\n",
      "score: 0.679514 (epoch no 124)\n",
      "score: 0.680105 (epoch no 125)\n",
      "score: 0.680440 (epoch no 126)\n",
      "score: 0.680812 (epoch no 127)\n",
      "score: 0.681348 (epoch no 128)\n",
      "score: 0.681899 (epoch no 129)\n",
      "score: 0.682124 (epoch no 130)\n",
      "score: 0.682388 (epoch no 131)\n",
      "score: 0.682740 (epoch no 132)\n",
      "score: 0.683241 (epoch no 133)\n",
      "score: 0.683684 (epoch no 134)\n",
      "score: 0.684295 (epoch no 135)\n",
      "score: 0.684646 (epoch no 136)\n",
      "score: 0.685013 (epoch no 137)\n",
      "score: 0.685535 (epoch no 138)\n",
      "score: 0.685671 (epoch no 139)\n",
      "score: 0.686261 (epoch no 140)\n",
      "score: 0.686775 (epoch no 141)\n",
      "score: 0.686986 (epoch no 142)\n",
      "score: 0.687489 (epoch no 143)\n",
      "score: 0.687635 (epoch no 144)\n",
      "score: 0.687983 (epoch no 145)\n",
      "score: 0.688179 (epoch no 146)\n",
      "score: 0.688775 (epoch no 147)\n",
      "score: 0.689118 (epoch no 148)\n",
      "score: 0.689403 (epoch no 149)\n",
      "score: 0.689755 (epoch no 150)\n",
      "score: 0.690121 (epoch no 151)\n",
      "score: 0.690421 (epoch no 152)\n",
      "score: 0.690740 (epoch no 153)\n",
      "score: 0.690944 (epoch no 154)\n",
      "score: 0.691427 (epoch no 155)\n",
      "score: 0.691885 (epoch no 156)\n",
      "score: 0.692109 (epoch no 157)\n",
      "score: 0.692438 (epoch no 158)\n",
      "score: 0.692596 (epoch no 159)\n",
      "score: 0.692782 (epoch no 160)\n",
      "score: 0.693227 (epoch no 161)\n",
      "score: 0.693404 (epoch no 162)\n",
      "score: 0.693589 (epoch no 163)\n",
      "score: 0.694014 (epoch no 164)\n",
      "score: 0.694437 (epoch no 165)\n",
      "score: 0.694594 (epoch no 166)\n",
      "score: 0.694633 (epoch no 167)\n",
      "score: 0.694959 (epoch no 168)\n",
      "score: 0.695274 (epoch no 169)\n",
      "score: 0.695868 (epoch no 170)\n",
      "score: 0.695920 (epoch no 171)\n",
      "score: 0.696128 (epoch no 172)\n",
      "score: 0.696380 (epoch no 173)\n",
      "score: 0.696606 (epoch no 174)\n",
      "score: 0.697011 (epoch no 175)\n",
      "score: 0.697104 (epoch no 176)\n",
      "score: 0.697366 (epoch no 177)\n",
      "score: 0.697658 (epoch no 178)\n",
      "score: 0.697883 (epoch no 179)\n",
      "score: 0.698022 (epoch no 180)\n",
      "score: 0.698242 (epoch no 181)\n",
      "score: 0.698328 (epoch no 182)\n",
      "score: 0.698654 (epoch no 183)\n",
      "score: 0.698880 (epoch no 184)\n",
      "score: 0.699192 (epoch no 185)\n",
      "score: 0.699604 (epoch no 186)\n",
      "score: 0.699563 (epoch no 187)\n",
      "score: 0.699885 (epoch no 188)\n",
      "score: 0.700015 (epoch no 189)\n",
      "score: 0.700387 (epoch no 190)\n",
      "score: 0.700421 (epoch no 191)\n",
      "score: 0.700615 (epoch no 192)\n",
      "score: 0.700818 (epoch no 193)\n",
      "score: 0.701138 (epoch no 194)\n",
      "score: 0.701446 (epoch no 195)\n",
      "score: 0.701476 (epoch no 196)\n",
      "score: 0.701681 (epoch no 197)\n",
      "score: 0.701740 (epoch no 198)\n",
      "score: 0.702066 (epoch no 199)\n",
      "score: 0.702222 (epoch no 200)\n",
      "score: 0.702569 (epoch no 201)\n",
      "score: 0.702584 (epoch no 202)\n",
      "score: 0.702768 (epoch no 203)\n",
      "score: 0.703148 (epoch no 204)\n",
      "score: 0.703264 (epoch no 205)\n",
      "score: 0.703405 (epoch no 206)\n",
      "score: 0.703536 (epoch no 207)\n",
      "score: 0.703850 (epoch no 208)\n",
      "score: 0.704024 (epoch no 209)\n",
      "score: 0.704229 (epoch no 210)\n",
      "score: 0.704405 (epoch no 211)\n",
      "score: 0.704606 (epoch no 212)\n",
      "score: 0.704765 (epoch no 213)\n",
      "score: 0.704804 (epoch no 214)\n",
      "score: 0.704974 (epoch no 215)\n",
      "score: 0.705132 (epoch no 216)\n",
      "score: 0.705307 (epoch no 217)\n",
      "score: 0.705646 (epoch no 218)\n",
      "score: 0.705826 (epoch no 219)\n",
      "score: 0.706023 (epoch no 220)\n",
      "score: 0.706062 (epoch no 221)\n",
      "score: 0.706196 (epoch no 222)\n",
      "score: 0.706372 (epoch no 223)\n",
      "score: 0.706449 (epoch no 224)\n",
      "score: 0.706658 (epoch no 225)\n",
      "score: 0.706778 (epoch no 226)\n",
      "score: 0.706884 (epoch no 227)\n",
      "score: 0.707162 (epoch no 228)\n",
      "score: 0.707254 (epoch no 229)\n",
      "score: 0.707324 (epoch no 230)\n",
      "score: 0.707380 (epoch no 231)\n",
      "score: 0.707659 (epoch no 232)\n",
      "score: 0.707722 (epoch no 233)\n",
      "score: 0.707618 (epoch no 234)\n",
      "score: 0.707882 (epoch no 235)\n",
      "score: 0.708047 (epoch no 236)\n",
      "score: 0.708185 (epoch no 237)\n",
      "score: 0.708438 (epoch no 238)\n",
      "score: 0.708663 (epoch no 239)\n",
      "score: 0.708534 (epoch no 240)\n",
      "score: 0.708677 (epoch no 241)\n",
      "score: 0.709010 (epoch no 242)\n",
      "score: 0.709135 (epoch no 243)\n",
      "score: 0.709175 (epoch no 244)\n",
      "score: 0.709396 (epoch no 245)\n",
      "score: 0.709647 (epoch no 246)\n",
      "score: 0.709722 (epoch no 247)\n",
      "score: 0.709670 (epoch no 248)\n",
      "score: 0.709784 (epoch no 249)\n",
      "score: 0.709839 (epoch no 250)\n",
      "score: 0.710032 (epoch no 251)\n",
      "score: 0.710218 (epoch no 252)\n",
      "score: 0.710300 (epoch no 253)\n",
      "score: 0.710347 (epoch no 254)\n",
      "score: 0.710552 (epoch no 255)\n",
      "score: 0.710626 (epoch no 256)\n",
      "score: 0.710759 (epoch no 257)\n",
      "score: 0.710987 (epoch no 258)\n",
      "score: 0.711070 (epoch no 259)\n",
      "score: 0.711124 (epoch no 260)\n",
      "score: 0.711143 (epoch no 261)\n",
      "score: 0.711398 (epoch no 262)\n",
      "score: 0.711434 (epoch no 263)\n",
      "score: 0.711522 (epoch no 264)\n",
      "score: 0.711471 (epoch no 265)\n",
      "score: 0.711561 (epoch no 266)\n",
      "score: 0.711642 (epoch no 267)\n",
      "score: 0.711917 (epoch no 268)\n",
      "score: 0.712050 (epoch no 269)\n",
      "score: 0.712064 (epoch no 270)\n",
      "score: 0.712125 (epoch no 271)\n",
      "score: 0.712222 (epoch no 272)\n",
      "score: 0.712257 (epoch no 273)\n",
      "score: 0.712249 (epoch no 274)\n",
      "score: 0.712520 (epoch no 275)\n",
      "score: 0.712603 (epoch no 276)\n",
      "score: 0.712679 (epoch no 277)\n",
      "score: 0.712768 (epoch no 278)\n",
      "score: 0.712884 (epoch no 279)\n",
      "score: 0.713022 (epoch no 280)\n",
      "score: 0.713039 (epoch no 281)\n",
      "score: 0.713227 (epoch no 282)\n",
      "score: 0.713355 (epoch no 283)\n",
      "score: 0.713401 (epoch no 284)\n",
      "score: 0.713309 (epoch no 285)\n",
      "score: 0.713473 (epoch no 286)\n",
      "score: 0.713542 (epoch no 287)\n",
      "score: 0.713636 (epoch no 288)\n",
      "score: 0.713753 (epoch no 289)\n",
      "score: 0.713817 (epoch no 290)\n",
      "score: 0.713943 (epoch no 291)\n",
      "score: 0.714124 (epoch no 292)\n",
      "score: 0.714227 (epoch no 293)\n",
      "score: 0.714233 (epoch no 294)\n",
      "score: 0.714302 (epoch no 295)\n",
      "score: 0.714393 (epoch no 296)\n",
      "score: 0.714455 (epoch no 297)\n",
      "score: 0.714485 (epoch no 298)\n",
      "score: 0.714591 (epoch no 299)\n",
      "score: 0.714454 (epoch no 300)\n",
      "score: 0.714621 (epoch no 301)\n",
      "score: 0.714866 (epoch no 302)\n",
      "score: 0.714982 (epoch no 303)\n",
      "score: 0.715001 (epoch no 304)\n",
      "score: 0.714973 (epoch no 305)\n",
      "score: 0.715151 (epoch no 306)\n",
      "score: 0.715269 (epoch no 307)\n",
      "score: 0.715356 (epoch no 308)\n",
      "score: 0.715400 (epoch no 309)\n",
      "score: 0.715449 (epoch no 310)\n",
      "score: 0.715564 (epoch no 311)\n",
      "score: 0.715594 (epoch no 312)\n",
      "score: 0.715584 (epoch no 313)\n",
      "score: 0.715668 (epoch no 314)\n",
      "score: 0.715855 (epoch no 315)\n",
      "score: 0.715855 (epoch no 316)\n",
      "score: 0.715881 (epoch no 317)\n",
      "score: 0.715914 (epoch no 318)\n",
      "score: 0.715934 (epoch no 319)\n",
      "score: 0.716015 (epoch no 320)\n",
      "score: 0.716079 (epoch no 321)\n",
      "score: 0.716189 (epoch no 322)\n",
      "score: 0.716235 (epoch no 323)\n",
      "score: 0.716373 (epoch no 324)\n",
      "score: 0.716389 (epoch no 325)\n",
      "score: 0.716302 (epoch no 326)\n",
      "score: 0.716420 (epoch no 327)\n",
      "score: 0.716466 (epoch no 328)\n",
      "score: 0.716518 (epoch no 329)\n",
      "score: 0.716564 (epoch no 330)\n",
      "score: 0.716692 (epoch no 331)\n",
      "score: 0.716754 (epoch no 332)\n",
      "score: 0.716797 (epoch no 333)\n",
      "score: 0.716948 (epoch no 334)\n",
      "score: 0.716996 (epoch no 335)\n",
      "score: 0.716936 (epoch no 336)\n",
      "score: 0.716952 (epoch no 337)\n",
      "score: 0.717126 (epoch no 338)\n",
      "score: 0.717099 (epoch no 339)\n",
      "score: 0.717270 (epoch no 340)\n",
      "score: 0.717308 (epoch no 341)\n",
      "score: 0.717335 (epoch no 342)\n",
      "score: 0.717437 (epoch no 343)\n",
      "score: 0.717391 (epoch no 344)\n",
      "score: 0.717506 (epoch no 345)\n",
      "score: 0.717657 (epoch no 346)\n",
      "score: 0.717728 (epoch no 347)\n",
      "score: 0.717703 (epoch no 348)\n",
      "score: 0.717740 (epoch no 349)\n",
      "score: 0.717736 (epoch no 350)\n",
      "score: 0.717733 (epoch no 351)\n",
      "score: 0.717732 (epoch no 352)\n",
      "score: 0.717764 (epoch no 353)\n",
      "score: 0.717756 (epoch no 354)\n",
      "score: 0.717788 (epoch no 355)\n",
      "score: 0.717865 (epoch no 356)\n",
      "score: 0.717925 (epoch no 357)\n",
      "score: 0.717973 (epoch no 358)\n",
      "score: 0.718079 (epoch no 359)\n",
      "score: 0.718214 (epoch no 360)\n",
      "score: 0.718232 (epoch no 361)\n",
      "score: 0.718173 (epoch no 362)\n",
      "score: 0.718322 (epoch no 363)\n",
      "score: 0.718286 (epoch no 364)\n",
      "score: 0.718387 (epoch no 365)\n",
      "score: 0.718394 (epoch no 366)\n",
      "score: 0.718587 (epoch no 367)\n",
      "score: 0.718549 (epoch no 368)\n",
      "score: 0.718675 (epoch no 369)\n",
      "score: 0.718704 (epoch no 370)\n",
      "score: 0.718855 (epoch no 371)\n",
      "score: 0.718744 (epoch no 372)\n",
      "score: 0.718711 (epoch no 373)\n",
      "score: 0.718763 (epoch no 374)\n",
      "score: 0.718862 (epoch no 375)\n",
      "score: 0.718874 (epoch no 376)\n",
      "score: 0.718870 (epoch no 377)\n",
      "score: 0.718909 (epoch no 378)\n",
      "score: 0.719030 (epoch no 379)\n",
      "score: 0.719052 (epoch no 380)\n",
      "score: 0.719035 (epoch no 381)\n",
      "score: 0.719131 (epoch no 382)\n",
      "score: 0.719127 (epoch no 383)\n",
      "score: 0.719184 (epoch no 384)\n",
      "score: 0.719291 (epoch no 385)\n",
      "score: 0.719337 (epoch no 386)\n",
      "score: 0.719305 (epoch no 387)\n",
      "score: 0.719437 (epoch no 388)\n",
      "score: 0.719410 (epoch no 389)\n",
      "score: 0.719372 (epoch no 390)\n",
      "score: 0.719390 (epoch no 391)\n",
      "score: 0.719527 (epoch no 392)\n",
      "score: 0.719532 (epoch no 393)\n",
      "score: 0.719544 (epoch no 394)\n",
      "score: 0.719650 (epoch no 395)\n",
      "score: 0.719656 (epoch no 396)\n",
      "score: 0.719674 (epoch no 397)\n",
      "score: 0.719704 (epoch no 398)\n",
      "score: 0.719735 (epoch no 399)\n",
      "score: 0.719749 (epoch no 400)\n",
      "score: 0.719715 (epoch no 401)\n",
      "score: 0.719749 (epoch no 402)\n",
      "score: 0.719755 (epoch no 403)\n",
      "score: 0.719849 (epoch no 404)\n",
      "score: 0.719948 (epoch no 405)\n",
      "score: 0.719988 (epoch no 406)\n",
      "score: 0.720030 (epoch no 407)\n",
      "score: 0.720069 (epoch no 408)\n",
      "score: 0.720124 (epoch no 409)\n",
      "score: 0.720175 (epoch no 410)\n",
      "score: 0.720140 (epoch no 411)\n",
      "score: 0.720219 (epoch no 412)\n",
      "score: 0.720196 (epoch no 413)\n",
      "score: 0.720151 (epoch no 414)\n",
      "score: 0.720174 (epoch no 415)\n",
      "score: 0.720214 (epoch no 416)\n",
      "score: 0.720130 (epoch no 417)\n",
      "score: 0.720155 (epoch no 418)\n",
      "score: 0.720275 (epoch no 419)\n",
      "score: 0.720343 (epoch no 420)\n",
      "score: 0.720360 (epoch no 421)\n",
      "score: 0.720508 (epoch no 422)\n",
      "score: 0.720568 (epoch no 423)\n",
      "score: 0.720524 (epoch no 424)\n",
      "score: 0.720476 (epoch no 425)\n",
      "score: 0.720550 (epoch no 426)\n",
      "score: 0.720519 (epoch no 427)\n",
      "score: 0.720573 (epoch no 428)\n",
      "score: 0.720632 (epoch no 429)\n",
      "score: 0.720652 (epoch no 430)\n",
      "score: 0.720645 (epoch no 431)\n",
      "score: 0.720693 (epoch no 432)\n",
      "score: 0.720675 (epoch no 433)\n",
      "score: 0.720757 (epoch no 434)\n",
      "score: 0.720722 (epoch no 435)\n",
      "score: 0.720748 (epoch no 436)\n",
      "score: 0.720716 (epoch no 437)\n",
      "score: 0.720884 (epoch no 438)\n",
      "score: 0.720921 (epoch no 439)\n",
      "score: 0.720985 (epoch no 440)\n",
      "score: 0.721010 (epoch no 441)\n",
      "score: 0.721027 (epoch no 442)\n",
      "score: 0.721011 (epoch no 443)\n",
      "score: 0.721007 (epoch no 444)\n",
      "score: 0.721019 (epoch no 445)\n",
      "score: 0.720944 (epoch no 446)\n",
      "score: 0.720994 (epoch no 447)\n",
      "score: 0.720944 (epoch no 448)\n",
      "score: 0.721055 (epoch no 449)\n",
      "score: 0.721004 (epoch no 450)\n",
      "score: 0.721001 (epoch no 451)\n",
      "score: 0.721060 (epoch no 452)\n",
      "score: 0.721170 (epoch no 453)\n",
      "score: 0.721226 (epoch no 454)\n",
      "score: 0.721185 (epoch no 455)\n",
      "score: 0.721242 (epoch no 456)\n",
      "score: 0.721295 (epoch no 457)\n",
      "score: 0.721347 (epoch no 458)\n",
      "score: 0.721375 (epoch no 459)\n",
      "score: 0.721466 (epoch no 460)\n",
      "score: 0.721411 (epoch no 461)\n",
      "score: 0.721409 (epoch no 462)\n",
      "score: 0.721515 (epoch no 463)\n",
      "score: 0.721474 (epoch no 464)\n",
      "score: 0.721396 (epoch no 465)\n",
      "score: 0.721367 (epoch no 466)\n",
      "score: 0.721398 (epoch no 467)\n",
      "score: 0.721471 (epoch no 468)\n",
      "score: 0.721400 (epoch no 469)\n",
      "score: 0.721420 (epoch no 470)\n",
      "score: 0.721435 (epoch no 471)\n",
      "score: 0.721449 (epoch no 472)\n",
      "score: 0.721499 (epoch no 473)\n",
      "score: 0.721480 (epoch no 474)\n",
      "score: 0.721513 (epoch no 475)\n",
      "score: 0.721497 (epoch no 476)\n",
      "score: 0.721516 (epoch no 477)\n",
      "score: 0.721539 (epoch no 478)\n",
      "score: 0.721483 (epoch no 479)\n",
      "score: 0.721558 (epoch no 480)\n",
      "score: 0.721580 (epoch no 481)\n",
      "score: 0.721666 (epoch no 482)\n",
      "score: 0.721731 (epoch no 483)\n",
      "score: 0.721725 (epoch no 484)\n",
      "score: 0.721728 (epoch no 485)\n",
      "score: 0.721700 (epoch no 486)\n",
      "score: 0.721653 (epoch no 487)\n",
      "score: 0.721656 (epoch no 488)\n",
      "score: 0.721726 (epoch no 489)\n",
      "score: 0.721794 (epoch no 490)\n",
      "score: 0.721898 (epoch no 491)\n",
      "score: 0.721844 (epoch no 492)\n",
      "score: 0.721888 (epoch no 493)\n",
      "score: 0.721871 (epoch no 494)\n",
      "score: 0.721899 (epoch no 495)\n",
      "score: 0.721862 (epoch no 496)\n",
      "score: 0.721929 (epoch no 497)\n",
      "score: 0.721962 (epoch no 498)\n",
      "score: 0.721955 (epoch no 499)\n",
      "score: 0.721965 (epoch no 500)\n",
      "score: 0.721925 (epoch no 501)\n",
      "score: 0.721979 (epoch no 502)\n",
      "score: 0.721989 (epoch no 503)\n",
      "score: 0.721926 (epoch no 504)\n",
      "score: 0.721915 (epoch no 505)\n",
      "score: 0.722001 (epoch no 506)\n",
      "score: 0.721993 (epoch no 507)\n",
      "score: 0.722075 (epoch no 508)\n",
      "score: 0.721999 (epoch no 509)\n",
      "score: 0.721956 (epoch no 510)\n",
      "score: 0.721907 (epoch no 511)\n",
      "score: 0.721904 (epoch no 512)\n",
      "score: 0.721927 (epoch no 513)\n",
      "score: 0.721886 (epoch no 514)\n",
      "score: 0.721955 (epoch no 515)\n",
      "score: 0.722014 (epoch no 516)\n",
      "score: 0.722002 (epoch no 517)\n",
      "score: 0.721949 (epoch no 518)\n",
      "score: 0.722002 (epoch no 519)\n",
      "score: 0.722111 (epoch no 520)\n",
      "score: 0.722089 (epoch no 521)\n",
      "score: 0.722127 (epoch no 522)\n",
      "score: 0.722123 (epoch no 523)\n",
      "score: 0.722109 (epoch no 524)\n",
      "score: 0.722078 (epoch no 525)\n",
      "score: 0.722096 (epoch no 526)\n",
      "score: 0.722161 (epoch no 527)\n",
      "score: 0.722196 (epoch no 528)\n",
      "score: 0.722171 (epoch no 529)\n",
      "score: 0.722235 (epoch no 530)\n",
      "score: 0.722185 (epoch no 531)\n",
      "score: 0.722202 (epoch no 532)\n",
      "score: 0.722228 (epoch no 533)\n",
      "score: 0.722284 (epoch no 534)\n",
      "score: 0.722292 (epoch no 535)\n",
      "score: 0.722195 (epoch no 536)\n",
      "score: 0.722254 (epoch no 537)\n",
      "score: 0.722290 (epoch no 538)\n",
      "score: 0.722327 (epoch no 539)\n",
      "score: 0.722435 (epoch no 540)\n",
      "score: 0.722372 (epoch no 541)\n",
      "score: 0.722351 (epoch no 542)\n",
      "score: 0.722401 (epoch no 543)\n",
      "score: 0.722358 (epoch no 544)\n",
      "score: 0.722419 (epoch no 545)\n",
      "score: 0.722451 (epoch no 546)\n",
      "score: 0.722452 (epoch no 547)\n",
      "score: 0.722429 (epoch no 548)\n",
      "score: 0.722399 (epoch no 549)\n",
      "score: 0.722420 (epoch no 550)\n",
      "score: 0.722460 (epoch no 551)\n",
      "score: 0.722408 (epoch no 552)\n",
      "score: 0.722455 (epoch no 553)\n",
      "score: 0.722484 (epoch no 554)\n",
      "score: 0.722490 (epoch no 555)\n",
      "score: 0.722510 (epoch no 556)\n",
      "score: 0.722539 (epoch no 557)\n",
      "score: 0.722557 (epoch no 558)\n",
      "score: 0.722568 (epoch no 559)\n",
      "score: 0.722584 (epoch no 560)\n",
      "score: 0.722621 (epoch no 561)\n",
      "score: 0.722629 (epoch no 562)\n",
      "score: 0.722551 (epoch no 563)\n",
      "score: 0.722539 (epoch no 564)\n",
      "score: 0.722535 (epoch no 565)\n",
      "score: 0.722502 (epoch no 566)\n",
      "score: 0.722530 (epoch no 567)\n",
      "score: 0.722591 (epoch no 568)\n",
      "score: 0.722619 (epoch no 569)\n",
      "score: 0.722646 (epoch no 570)\n",
      "score: 0.722644 (epoch no 571)\n",
      "score: 0.722582 (epoch no 572)\n",
      "score: 0.722498 (epoch no 573)\n",
      "score: 0.722597 (epoch no 574)\n",
      "score: 0.722569 (epoch no 575)\n",
      "score: 0.722617 (epoch no 576)\n",
      "score: 0.722590 (epoch no 577)\n",
      "score: 0.722578 (epoch no 578)\n",
      "score: 0.722596 (epoch no 579)\n",
      "score: 0.722691 (epoch no 580)\n",
      "score: 0.722715 (epoch no 581)\n",
      "score: 0.722665 (epoch no 582)\n",
      "score: 0.722628 (epoch no 583)\n",
      "score: 0.722623 (epoch no 584)\n",
      "score: 0.722604 (epoch no 585)\n",
      "score: 0.722628 (epoch no 586)\n",
      "score: 0.722664 (epoch no 587)\n",
      "score: 0.722643 (epoch no 588)\n",
      "score: 0.722740 (epoch no 589)\n",
      "score: 0.722706 (epoch no 590)\n",
      "score: 0.722706 (epoch no 591)\n",
      "score: 0.722635 (epoch no 592)\n",
      "score: 0.722692 (epoch no 593)\n",
      "score: 0.722705 (epoch no 594)\n",
      "score: 0.722755 (epoch no 595)\n",
      "score: 0.722765 (epoch no 596)\n",
      "score: 0.722712 (epoch no 597)\n",
      "score: 0.722668 (epoch no 598)\n",
      "score: 0.722652 (epoch no 599)\n",
      "score: 0.722708 (epoch no 600)\n",
      "score: 0.722770 (epoch no 601)\n",
      "score: 0.722803 (epoch no 602)\n",
      "score: 0.722758 (epoch no 603)\n",
      "score: 0.722749 (epoch no 604)\n",
      "score: 0.722755 (epoch no 605)\n",
      "score: 0.722773 (epoch no 606)\n",
      "score: 0.722844 (epoch no 607)\n",
      "score: 0.722834 (epoch no 608)\n",
      "score: 0.722797 (epoch no 609)\n",
      "score: 0.722822 (epoch no 610)\n",
      "score: 0.722851 (epoch no 611)\n",
      "score: 0.722791 (epoch no 612)\n",
      "score: 0.722849 (epoch no 613)\n",
      "score: 0.722841 (epoch no 614)\n",
      "score: 0.722778 (epoch no 615)\n",
      "score: 0.722862 (epoch no 616)\n",
      "score: 0.722824 (epoch no 617)\n",
      "score: 0.722789 (epoch no 618)\n",
      "score: 0.722851 (epoch no 619)\n",
      "score: 0.722806 (epoch no 620)\n",
      "score: 0.722862 (epoch no 621)\n",
      "score: 0.722824 (epoch no 622)\n",
      "score: 0.722848 (epoch no 623)\n",
      "score: 0.722823 (epoch no 624)\n",
      "score: 0.722807 (epoch no 625)\n",
      "score: 0.722760 (epoch no 626)\n",
      "score: 0.722803 (epoch no 627)\n",
      "score: 0.722830 (epoch no 628)\n",
      "score: 0.722836 (epoch no 629)\n",
      "score: 0.722852 (epoch no 630)\n",
      "score: 0.722817 (epoch no 631)\n",
      "score: 0.722884 (epoch no 632)\n",
      "score: 0.722859 (epoch no 633)\n",
      "score: 0.722820 (epoch no 634)\n",
      "score: 0.722834 (epoch no 635)\n",
      "score: 0.722867 (epoch no 636)\n",
      "score: 0.722856 (epoch no 637)\n",
      "score: 0.722886 (epoch no 638)\n",
      "score: 0.722886 (epoch no 639)\n",
      "score: 0.722886 (epoch no 640)\n",
      "score: 0.722805 (epoch no 641)\n",
      "score: 0.722779 (epoch no 642)\n",
      "score: 0.722769 (epoch no 643)\n",
      "score: 0.722813 (epoch no 644)\n",
      "score: 0.722848 (epoch no 645)\n",
      "score: 0.722875 (epoch no 646)\n",
      "score: 0.722767 (epoch no 647)\n",
      "score: 0.722798 (epoch no 648)\n",
      "score: 0.722808 (epoch no 649)\n",
      "score: 0.722755 (epoch no 650)\n",
      "score: 0.722780 (epoch no 651)\n",
      "score: 0.722801 (epoch no 652)\n",
      "score: 0.722809 (epoch no 653)\n",
      "score: 0.722774 (epoch no 654)\n",
      "score: 0.722803 (epoch no 655)\n",
      "score: 0.722811 (epoch no 656)\n",
      "score: 0.722825 (epoch no 657)\n",
      "score: 0.722914 (epoch no 658)\n",
      "score: 0.722879 (epoch no 659)\n",
      "score: 0.722822 (epoch no 660)\n",
      "score: 0.722936 (epoch no 661)\n",
      "score: 0.722990 (epoch no 662)\n",
      "score: 0.722921 (epoch no 663)\n",
      "score: 0.722880 (epoch no 664)\n",
      "score: 0.722866 (epoch no 665)\n",
      "score: 0.722817 (epoch no 666)\n",
      "score: 0.722844 (epoch no 667)\n",
      "score: 0.722847 (epoch no 668)\n",
      "score: 0.722851 (epoch no 669)\n",
      "score: 0.722831 (epoch no 670)\n",
      "score: 0.722831 (epoch no 671)\n",
      "score: 0.722776 (epoch no 672)\n",
      "score: 0.722856 (epoch no 673)\n",
      "score: 0.722918 (epoch no 674)\n",
      "score: 0.722883 (epoch no 675)\n",
      "score: 0.722878 (epoch no 676)\n",
      "score: 0.722888 (epoch no 677)\n",
      "score: 0.722860 (epoch no 678)\n",
      "score: 0.722838 (epoch no 679)\n",
      "score: 0.722821 (epoch no 680)\n",
      "score: 0.722844 (epoch no 681)\n",
      "score: 0.722924 (epoch no 682)\n",
      "score: 0.722944 (epoch no 683)\n",
      "score: 0.722975 (epoch no 684)\n",
      "score: 0.722938 (epoch no 685)\n",
      "score: 0.722937 (epoch no 686)\n",
      "score: 0.722916 (epoch no 687)\n",
      "score: 0.722859 (epoch no 688)\n",
      "score: 0.722845 (epoch no 689)\n",
      "score: 0.722772 (epoch no 690)\n",
      "score: 0.722762 (epoch no 691)\n",
      "score: 0.722785 (epoch no 692)\n",
      "score: 0.722801 (epoch no 693)\n",
      "score: 0.722731 (epoch no 694)\n",
      "score: 0.722782 (epoch no 695)\n",
      "score: 0.722773 (epoch no 696)\n",
      "score: 0.722765 (epoch no 697)\n",
      "score: 0.722756 (epoch no 698)\n",
      "score: 0.722708 (epoch no 699)\n",
      "score: 0.722658 (epoch no 700)\n",
      "score: 0.722603 (epoch no 701)\n",
      "score: 0.722605 (epoch no 702)\n",
      "score: 0.722660 (epoch no 703)\n",
      "score: 0.722624 (epoch no 704)\n",
      "score: 0.722687 (epoch no 705)\n",
      "score: 0.722778 (epoch no 706)\n",
      "score: 0.722756 (epoch no 707)\n",
      "score: 0.722783 (epoch no 708)\n",
      "score: 0.722741 (epoch no 709)\n",
      "score: 0.722681 (epoch no 710)\n",
      "score: 0.722683 (epoch no 711)\n",
      "score: 0.722647 (epoch no 712)\n",
      "score: 0.722647 (epoch no 713)\n",
      "score: 0.722654 (epoch no 714)\n",
      "score: 0.722724 (epoch no 715)\n",
      "score: 0.722727 (epoch no 716)\n",
      "score: 0.722637 (epoch no 717)\n",
      "score: 0.722704 (epoch no 718)\n",
      "score: 0.722746 (epoch no 719)\n",
      "score: 0.722727 (epoch no 720)\n",
      "score: 0.722703 (epoch no 721)\n",
      "score: 0.722721 (epoch no 722)\n",
      "score: 0.722696 (epoch no 723)\n",
      "score: 0.722635 (epoch no 724)\n",
      "score: 0.722666 (epoch no 725)\n",
      "score: 0.722692 (epoch no 726)\n",
      "score: 0.722700 (epoch no 727)\n",
      "score: 0.722711 (epoch no 728)\n",
      "score: 0.722653 (epoch no 729)\n",
      "score: 0.722659 (epoch no 730)\n",
      "score: 0.722608 (epoch no 731)\n",
      "score: 0.722662 (epoch no 732)\n",
      "score: 0.722683 (epoch no 733)\n",
      "score: 0.722670 (epoch no 734)\n",
      "score: 0.722657 (epoch no 735)\n",
      "score: 0.722591 (epoch no 736)\n",
      "score: 0.722589 (epoch no 737)\n",
      "score: 0.722587 (epoch no 738)\n",
      "score: 0.722623 (epoch no 739)\n",
      "score: 0.722531 (epoch no 740)\n",
      "score: 0.722586 (epoch no 741)\n",
      "score: 0.722588 (epoch no 742)\n",
      "score: 0.722651 (epoch no 743)\n",
      "score: 0.722656 (epoch no 744)\n",
      "score: 0.722600 (epoch no 745)\n",
      "score: 0.722578 (epoch no 746)\n",
      "score: 0.722625 (epoch no 747)\n",
      "score: 0.722623 (epoch no 748)\n",
      "score: 0.722594 (epoch no 749)\n",
      "score: 0.722584 (epoch no 750)\n",
      "score: 0.722590 (epoch no 751)\n",
      "score: 0.722599 (epoch no 752)\n",
      "score: 0.722640 (epoch no 753)\n",
      "score: 0.722650 (epoch no 754)\n",
      "score: 0.722581 (epoch no 755)\n",
      "score: 0.722622 (epoch no 756)\n",
      "score: 0.722635 (epoch no 757)\n",
      "score: 0.722537 (epoch no 758)\n",
      "score: 0.722556 (epoch no 759)\n",
      "score: 0.722485 (epoch no 760)\n",
      "score: 0.722486 (epoch no 761)\n",
      "score: 0.722517 (epoch no 762)\n",
      "score: 0.722508 (epoch no 763)\n",
      "score: 0.722562 (epoch no 764)\n",
      "score: 0.722532 (epoch no 765)\n",
      "score: 0.722529 (epoch no 766)\n",
      "score: 0.722552 (epoch no 767)\n",
      "score: 0.722508 (epoch no 768)\n",
      "score: 0.722493 (epoch no 769)\n",
      "score: 0.722510 (epoch no 770)\n",
      "score: 0.722539 (epoch no 771)\n",
      "score: 0.722467 (epoch no 772)\n",
      "score: 0.722513 (epoch no 773)\n",
      "score: 0.722438 (epoch no 774)\n",
      "score: 0.722526 (epoch no 775)\n",
      "score: 0.722569 (epoch no 776)\n",
      "score: 0.722567 (epoch no 777)\n",
      "score: 0.722538 (epoch no 778)\n",
      "score: 0.722474 (epoch no 779)\n",
      "score: 0.722391 (epoch no 780)\n",
      "score: 0.722426 (epoch no 781)\n",
      "score: 0.722481 (epoch no 782)\n",
      "score: 0.722516 (epoch no 783)\n",
      "score: 0.722532 (epoch no 784)\n",
      "score: 0.722571 (epoch no 785)\n",
      "score: 0.722536 (epoch no 786)\n",
      "score: 0.722467 (epoch no 787)\n",
      "score: 0.722427 (epoch no 788)\n",
      "score: 0.722443 (epoch no 789)\n",
      "score: 0.722422 (epoch no 790)\n",
      "score: 0.722412 (epoch no 791)\n",
      "score: 0.722475 (epoch no 792)\n",
      "score: 0.722448 (epoch no 793)\n",
      "score: 0.722398 (epoch no 794)\n",
      "score: 0.722394 (epoch no 795)\n",
      "score: 0.722449 (epoch no 796)\n",
      "score: 0.722450 (epoch no 797)\n",
      "score: 0.722412 (epoch no 798)\n",
      "score: 0.722433 (epoch no 799)\n",
      "score: 0.722406 (epoch no 800)\n",
      "score: 0.722426 (epoch no 801)\n",
      "score: 0.722438 (epoch no 802)\n",
      "score: 0.722393 (epoch no 803)\n",
      "score: 0.722428 (epoch no 804)\n",
      "score: 0.722350 (epoch no 805)\n",
      "score: 0.722449 (epoch no 806)\n",
      "score: 0.722435 (epoch no 807)\n",
      "score: 0.722392 (epoch no 808)\n",
      "score: 0.722396 (epoch no 809)\n",
      "score: 0.722391 (epoch no 810)\n",
      "score: 0.722423 (epoch no 811)\n",
      "score: 0.722423 (epoch no 812)\n",
      "score: 0.722402 (epoch no 813)\n",
      "score: 0.722340 (epoch no 814)\n",
      "score: 0.722325 (epoch no 815)\n",
      "score: 0.722311 (epoch no 816)\n",
      "score: 0.722329 (epoch no 817)\n",
      "score: 0.722344 (epoch no 818)\n",
      "score: 0.722384 (epoch no 819)\n",
      "score: 0.722320 (epoch no 820)\n",
      "score: 0.722297 (epoch no 821)\n",
      "score: 0.722242 (epoch no 822)\n",
      "score: 0.722265 (epoch no 823)\n",
      "score: 0.722237 (epoch no 824)\n",
      "score: 0.722232 (epoch no 825)\n",
      "score: 0.722157 (epoch no 826)\n",
      "score: 0.722189 (epoch no 827)\n",
      "score: 0.722199 (epoch no 828)\n",
      "score: 0.722200 (epoch no 829)\n",
      "score: 0.722210 (epoch no 830)\n",
      "score: 0.722176 (epoch no 831)\n",
      "score: 0.722075 (epoch no 832)\n",
      "score: 0.722134 (epoch no 833)\n",
      "score: 0.722208 (epoch no 834)\n",
      "score: 0.722231 (epoch no 835)\n",
      "score: 0.722222 (epoch no 836)\n",
      "score: 0.722170 (epoch no 837)\n",
      "score: 0.722122 (epoch no 838)\n",
      "score: 0.722151 (epoch no 839)\n",
      "score: 0.722181 (epoch no 840)\n",
      "score: 0.722201 (epoch no 841)\n",
      "score: 0.722225 (epoch no 842)\n",
      "score: 0.722259 (epoch no 843)\n",
      "score: 0.722221 (epoch no 844)\n",
      "score: 0.722205 (epoch no 845)\n",
      "score: 0.722169 (epoch no 846)\n",
      "score: 0.722128 (epoch no 847)\n",
      "score: 0.722188 (epoch no 848)\n",
      "score: 0.722137 (epoch no 849)\n",
      "score: 0.722178 (epoch no 850)\n",
      "score: 0.722154 (epoch no 851)\n",
      "score: 0.722172 (epoch no 852)\n",
      "score: 0.722137 (epoch no 853)\n",
      "score: 0.722077 (epoch no 854)\n",
      "score: 0.722126 (epoch no 855)\n",
      "score: 0.722078 (epoch no 856)\n",
      "score: 0.722114 (epoch no 857)\n",
      "score: 0.722087 (epoch no 858)\n",
      "score: 0.722100 (epoch no 859)\n",
      "score: 0.722081 (epoch no 860)\n",
      "score: 0.722072 (epoch no 861)\n",
      "score: 0.722102 (epoch no 862)\n",
      "score: 0.722111 (epoch no 863)\n",
      "score: 0.722129 (epoch no 864)\n",
      "score: 0.722133 (epoch no 865)\n",
      "score: 0.722041 (epoch no 866)\n",
      "score: 0.722015 (epoch no 867)\n",
      "score: 0.722011 (epoch no 868)\n",
      "score: 0.722037 (epoch no 869)\n",
      "score: 0.722024 (epoch no 870)\n",
      "score: 0.722021 (epoch no 871)\n",
      "score: 0.721965 (epoch no 872)\n",
      "score: 0.721980 (epoch no 873)\n",
      "score: 0.721895 (epoch no 874)\n",
      "score: 0.721957 (epoch no 875)\n",
      "score: 0.721908 (epoch no 876)\n",
      "score: 0.721899 (epoch no 877)\n",
      "score: 0.721911 (epoch no 878)\n",
      "score: 0.721910 (epoch no 879)\n",
      "score: 0.721921 (epoch no 880)\n",
      "score: 0.721924 (epoch no 881)\n",
      "score: 0.721959 (epoch no 882)\n",
      "score: 0.721991 (epoch no 883)\n",
      "score: 0.721977 (epoch no 884)\n",
      "score: 0.721984 (epoch no 885)\n",
      "score: 0.721901 (epoch no 886)\n",
      "score: 0.721879 (epoch no 887)\n",
      "score: 0.721838 (epoch no 888)\n",
      "score: 0.721925 (epoch no 889)\n",
      "score: 0.721910 (epoch no 890)\n",
      "score: 0.721873 (epoch no 891)\n",
      "score: 0.721815 (epoch no 892)\n",
      "score: 0.721818 (epoch no 893)\n",
      "score: 0.721798 (epoch no 894)\n",
      "score: 0.721770 (epoch no 895)\n",
      "score: 0.721829 (epoch no 896)\n",
      "score: 0.721761 (epoch no 897)\n",
      "score: 0.721795 (epoch no 898)\n",
      "score: 0.721810 (epoch no 899)\n",
      "score: 0.721801 (epoch no 900)\n",
      "score: 0.721757 (epoch no 901)\n",
      "score: 0.721766 (epoch no 902)\n",
      "score: 0.721721 (epoch no 903)\n",
      "score: 0.721718 (epoch no 904)\n",
      "score: 0.721695 (epoch no 905)\n",
      "score: 0.721680 (epoch no 906)\n",
      "score: 0.721695 (epoch no 907)\n",
      "score: 0.721623 (epoch no 908)\n",
      "score: 0.721636 (epoch no 909)\n",
      "score: 0.721601 (epoch no 910)\n",
      "score: 0.721625 (epoch no 911)\n",
      "score: 0.721648 (epoch no 912)\n",
      "score: 0.721601 (epoch no 913)\n",
      "score: 0.721621 (epoch no 914)\n",
      "score: 0.721651 (epoch no 915)\n",
      "score: 0.721695 (epoch no 916)\n",
      "score: 0.721735 (epoch no 917)\n",
      "score: 0.721681 (epoch no 918)\n",
      "score: 0.721656 (epoch no 919)\n",
      "score: 0.721650 (epoch no 920)\n",
      "score: 0.721591 (epoch no 921)\n",
      "score: 0.721612 (epoch no 922)\n",
      "score: 0.721582 (epoch no 923)\n",
      "score: 0.721597 (epoch no 924)\n",
      "score: 0.721598 (epoch no 925)\n",
      "score: 0.721591 (epoch no 926)\n",
      "score: 0.721605 (epoch no 927)\n",
      "score: 0.721483 (epoch no 928)\n",
      "score: 0.721472 (epoch no 929)\n",
      "score: 0.721473 (epoch no 930)\n",
      "score: 0.721469 (epoch no 931)\n",
      "score: 0.721459 (epoch no 932)\n",
      "score: 0.721500 (epoch no 933)\n",
      "score: 0.721518 (epoch no 934)\n",
      "score: 0.721545 (epoch no 935)\n",
      "score: 0.721549 (epoch no 936)\n",
      "score: 0.721530 (epoch no 937)\n",
      "score: 0.721499 (epoch no 938)\n",
      "score: 0.721546 (epoch no 939)\n",
      "score: 0.721513 (epoch no 940)\n",
      "score: 0.721510 (epoch no 941)\n",
      "score: 0.721515 (epoch no 942)\n",
      "score: 0.721489 (epoch no 943)\n",
      "score: 0.721471 (epoch no 944)\n",
      "score: 0.721441 (epoch no 945)\n",
      "score: 0.721377 (epoch no 946)\n",
      "score: 0.721385 (epoch no 947)\n",
      "score: 0.721390 (epoch no 948)\n",
      "score: 0.721386 (epoch no 949)\n",
      "score: 0.721378 (epoch no 950)\n",
      "score: 0.721405 (epoch no 951)\n",
      "score: 0.721384 (epoch no 952)\n",
      "score: 0.721357 (epoch no 953)\n",
      "score: 0.721345 (epoch no 954)\n",
      "score: 0.721358 (epoch no 955)\n",
      "score: 0.721324 (epoch no 956)\n",
      "score: 0.721336 (epoch no 957)\n",
      "score: 0.721371 (epoch no 958)\n",
      "score: 0.721342 (epoch no 959)\n",
      "score: 0.721342 (epoch no 960)\n",
      "score: 0.721279 (epoch no 961)\n",
      "score: 0.721268 (epoch no 962)\n",
      "score: 0.721262 (epoch no 963)\n",
      "score: 0.721197 (epoch no 964)\n",
      "score: 0.721206 (epoch no 965)\n",
      "score: 0.721142 (epoch no 966)\n",
      "score: 0.721170 (epoch no 967)\n",
      "score: 0.721171 (epoch no 968)\n",
      "score: 0.721121 (epoch no 969)\n",
      "score: 0.721130 (epoch no 970)\n",
      "score: 0.721160 (epoch no 971)\n",
      "score: 0.721157 (epoch no 972)\n",
      "score: 0.721091 (epoch no 973)\n",
      "score: 0.721064 (epoch no 974)\n",
      "score: 0.721057 (epoch no 975)\n",
      "score: 0.721067 (epoch no 976)\n",
      "score: 0.721069 (epoch no 977)\n",
      "score: 0.721095 (epoch no 978)\n",
      "score: 0.721119 (epoch no 979)\n",
      "score: 0.721079 (epoch no 980)\n",
      "score: 0.721068 (epoch no 981)\n",
      "score: 0.721069 (epoch no 982)\n",
      "score: 0.721058 (epoch no 983)\n",
      "score: 0.721048 (epoch no 984)\n",
      "score: 0.721000 (epoch no 985)\n",
      "score: 0.721006 (epoch no 986)\n",
      "score: 0.721011 (epoch no 987)\n",
      "score: 0.721071 (epoch no 988)\n",
      "score: 0.721050 (epoch no 989)\n",
      "score: 0.721023 (epoch no 990)\n",
      "score: 0.720966 (epoch no 991)\n",
      "score: 0.720970 (epoch no 992)\n",
      "score: 0.720892 (epoch no 993)\n",
      "score: 0.720911 (epoch no 994)\n",
      "score: 0.720958 (epoch no 995)\n",
      "score: 0.720960 (epoch no 996)\n",
      "score: 0.720915 (epoch no 997)\n",
      "score: 0.720914 (epoch no 998)\n",
      "score: 0.720850 (epoch no 999)\n",
      "score: 0.720898 (epoch no 1000)\n",
      "score: 0.720916 (epoch no 1001)\n",
      "score: 0.720927 (epoch no 1002)\n",
      "score: 0.720895 (epoch no 1003)\n",
      "score: 0.720892 (epoch no 1004)\n",
      "score: 0.720902 (epoch no 1005)\n",
      "score: 0.720920 (epoch no 1006)\n",
      "score: 0.720899 (epoch no 1007)\n",
      "score: 0.720789 (epoch no 1008)\n",
      "score: 0.720833 (epoch no 1009)\n",
      "score: 0.720856 (epoch no 1010)\n",
      "score: 0.720826 (epoch no 1011)\n",
      "score: 0.720819 (epoch no 1012)\n",
      "score: 0.720758 (epoch no 1013)\n",
      "score: 0.720767 (epoch no 1014)\n",
      "score: 0.720788 (epoch no 1015)\n",
      "score: 0.720780 (epoch no 1016)\n",
      "score: 0.720753 (epoch no 1017)\n",
      "score: 0.720758 (epoch no 1018)\n",
      "score: 0.720698 (epoch no 1019)\n",
      "score: 0.720662 (epoch no 1020)\n",
      "score: 0.720675 (epoch no 1021)\n",
      "score: 0.720612 (epoch no 1022)\n",
      "score: 0.720620 (epoch no 1023)\n",
      "score: 0.720536 (epoch no 1024)\n",
      "score: 0.720547 (epoch no 1025)\n",
      "score: 0.720604 (epoch no 1026)\n",
      "score: 0.720595 (epoch no 1027)\n",
      "score: 0.720600 (epoch no 1028)\n",
      "score: 0.720579 (epoch no 1029)\n",
      "score: 0.720524 (epoch no 1030)\n",
      "score: 0.720483 (epoch no 1031)\n",
      "score: 0.720590 (epoch no 1032)\n",
      "score: 0.720559 (epoch no 1033)\n",
      "score: 0.720583 (epoch no 1034)\n",
      "score: 0.720589 (epoch no 1035)\n",
      "score: 0.720591 (epoch no 1036)\n",
      "score: 0.720584 (epoch no 1037)\n",
      "score: 0.720654 (epoch no 1038)\n",
      "score: 0.720623 (epoch no 1039)\n",
      "score: 0.720608 (epoch no 1040)\n",
      "score: 0.720620 (epoch no 1041)\n",
      "score: 0.720588 (epoch no 1042)\n",
      "score: 0.720541 (epoch no 1043)\n",
      "score: 0.720537 (epoch no 1044)\n",
      "score: 0.720575 (epoch no 1045)\n",
      "score: 0.720583 (epoch no 1046)\n",
      "score: 0.720574 (epoch no 1047)\n",
      "score: 0.720637 (epoch no 1048)\n",
      "score: 0.720598 (epoch no 1049)\n",
      "score: 0.720555 (epoch no 1050)\n",
      "score: 0.720506 (epoch no 1051)\n",
      "score: 0.720515 (epoch no 1052)\n",
      "score: 0.720463 (epoch no 1053)\n",
      "score: 0.720486 (epoch no 1054)\n",
      "score: 0.720483 (epoch no 1055)\n",
      "score: 0.720426 (epoch no 1056)\n",
      "score: 0.720414 (epoch no 1057)\n",
      "score: 0.720363 (epoch no 1058)\n",
      "score: 0.720404 (epoch no 1059)\n",
      "score: 0.720339 (epoch no 1060)\n",
      "score: 0.720365 (epoch no 1061)\n",
      "score: 0.720369 (epoch no 1062)\n",
      "score: 0.720325 (epoch no 1063)\n",
      "score: 0.720347 (epoch no 1064)\n",
      "score: 0.720329 (epoch no 1065)\n",
      "score: 0.720294 (epoch no 1066)\n",
      "score: 0.720289 (epoch no 1067)\n",
      "score: 0.720229 (epoch no 1068)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-4653f353a54e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m model.fit(X_train_svd, y[train].toarray(), nb_epoch=EPOCHS, batch_size=10000, callbacks=[watchlist],\n\u001b[1;32m----> 2\u001b[1;33m           verbose=0)\n\u001b[0m",
      "\u001b[1;32m/home/agrigorev/anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                               \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m                               sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m/home/agrigorev/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[0;32m   1009\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1010\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m                               callback_metrics=callback_metrics)\n\u001b[0m\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/agrigorev/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics)\u001b[0m\n\u001b[0;32m    768\u001b[0m                         \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m                             \u001b[0mepoch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 770\u001b[1;33m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    771\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    772\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/agrigorev/anaconda2/lib/python2.7/site-packages/keras/callbacks.pyc\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-297f9888956a>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch_no\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/agrigorev/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[0;32m   1082\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m         return self._predict_loop(f, ins,\n\u001b[1;32m-> 1084\u001b[1;33m                                   batch_size=batch_size, verbose=verbose)\n\u001b[0m\u001b[0;32m   1085\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1086\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32m/home/agrigorev/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_predict_loop\u001b[1;34m(self, f, ins, batch_size, verbose)\u001b[0m\n\u001b[0;32m    802\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_X\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    803\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 804\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    805\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m                 \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/agrigorev/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    486\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/agrigorev/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train_svd, y[train].toarray(), nb_epoch=EPOCHS, batch_size=10000, callbacks=[watchlist],\n",
    "          verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "EPOCHS = 600\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(input_dim=X_train_svd.shape[1], output_dim=300, init='uniform')) \n",
    "model.add(Activation('tanh')) \n",
    "model.add(Dropout(0.2)) \n",
    "model.add(Dense(input_dim=300, output_dim=150, init='uniform')) \n",
    "model.add(Activation('sigmoid')) \n",
    "model.add(Dropout(0.1)) \n",
    "model.add(Dense(output_dim=y.shape[1], init='uniform')) \n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adagrad')\n",
    "\n",
    "with StatusCallback(epoch_total=EPOCHS) as status:\n",
    "    model.fit(X, y.toarray(), nb_epoch=EPOCHS, batch_size=10000, callbacks=[status], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_valid_pred = model.predict(X_valid)\n",
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savetxt('submission/tania_valid.predict', y_valid_pred, fmt='%0.10f')\n",
    "np.savetxt('submission/tania_test.predict', y_test_pred, fmt='%0.10f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
